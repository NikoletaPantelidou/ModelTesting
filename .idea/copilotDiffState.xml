<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/GPU_SETUP.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/GPU_SETUP.md" />
              <option name="updatedContent" value="# Guía para Configurar GPU con PyTorch&#10;&#10;## Problema&#10;Tu PyTorch está instalado sin soporte CUDA, por lo que no detecta tu GPU.&#10;&#10;## Solución&#10;&#10;### Paso 1: Verificar tu GPU&#10;Ejecuta en PowerShell o CMD:&#10;```bash&#10;nvidia-smi&#10;```&#10;Esto te mostrará tu GPU NVIDIA y la versión de CUDA Driver instalada.&#10;&#10;### Paso 2: Instalar PyTorch con CUDA&#10;&#10;**Opción A - Ejecutar el script automático (RECOMENDADO):**&#10;```bash&#10;install_pytorch_gpu.bat&#10;```&#10;&#10;**Opción B - Instalación manual:**&#10;&#10;Para CUDA 11.8 (compatible con la mayoría de GPUs):&#10;```bash&#10;pip uninstall torch torchvision torchaudio -y&#10;pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118&#10;```&#10;&#10;Para CUDA 12.1 (GPUs más recientes):&#10;```bash&#10;pip uninstall torch torchvision torchaudio -y&#10;pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121&#10;```&#10;&#10;### Paso 3: Verificar la instalación&#10;```bash&#10;py check_gpu.py&#10;```&#10;&#10;Deberías ver:&#10;```&#10;CUDA available: True&#10;CUDA version: 11.8 (o 12.1)&#10;Device count: 1&#10;Device name: [Nombre de tu GPU]&#10;```&#10;&#10;### Paso 4: Ejecutar el script&#10;```bash&#10;py script.py&#10;```&#10;&#10;## Configuración de MAX_WORKERS&#10;&#10;El script está configurado con `MAX_WORKERS = 1` por defecto. Esto es **RECOMENDADO** para:&#10;- Modelos grandes (7B+ parámetros)&#10;- GPUs con menos de 24GB de memoria&#10;&#10;### ¿Cuándo aumentar MAX_WORKERS?&#10;&#10;Solo aumenta si:&#10;- Tienes una GPU con mucha memoria (24GB+)&#10;- Estás usando modelos pequeños (&lt;3B parámetros)&#10;- No ves errores &quot;CUDA out of memory&quot;&#10;&#10;**Valores recomendados:**&#10;- GPU 8GB (RTX 3060, etc.): `MAX_WORKERS = 1`&#10;- GPU 12GB (RTX 3080, etc.): `MAX_WORKERS = 1-2`&#10;- GPU 16GB (RTX 4080, etc.): `MAX_WORKERS = 2`&#10;- GPU 24GB+ (RTX 4090, A100, etc.): `MAX_WORKERS = 2-4`&#10;&#10;## Beneficios de usar GPU&#10;&#10;**Sin GPU (CPU):**&#10;- Velocidad: ~1-5 minutos por prompt&#10;- Tiempo total: ~2-4 horas para 50 prompts&#10;&#10;**Con GPU:**&#10;- Velocidad: ~5-15 segundos por prompt&#10;- Tiempo total: ~5-15 minutos para 50 prompts&#10;&#10;**¡Hasta 20x más rápido!**&#10;&#10;## Problemas comunes&#10;&#10;### &quot;CUDA out of memory&quot;&#10;- Reduce MAX_WORKERS a 1&#10;- Cierra otras aplicaciones que usen GPU&#10;- Usa modelos más pequeños&#10;&#10;### &quot;No GPU detected&quot; después de instalar&#10;- Verifica que tienes drivers NVIDIA actualizados&#10;- Reinicia el terminal/IDE&#10;- Verifica la versión de CUDA instalada con `nvidia-smi`&#10;&#10;### Modelo requiere FP8 quantization&#10;- Instala accelerate: `pip install accelerate`&#10;- Algunos modelos requieren GPUs específicas (H100, etc.)&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Ollama/CORRECCION_APLICADA.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Ollama/CORRECCION_APLICADA.md" />
              <option name="originalContent" value="#  CORRECCIÓN APLICADA - Ollama Local&#10;&#10;## ❌ Problema Identificado&#10;&#10;El script estaba intentando conectarse a `https://api.ollama.com/v1`, pero **esta API no existe**. &#10;&#10;El error en los logs era:&#10;```&#10;Failed to resolve 'api.ollama.com'&#10;```&#10;&#10;## ✅ Solución Aplicada&#10;&#10;He corregido el script para usar **Ollama Local** en lugar de una inexistente &quot;Ollama Cloud&quot;:&#10;&#10;### Cambios Realizados:&#10;&#10;1. **URL corregida**: &#10;   - ❌ Antes: `https://api.ollama.com/v1`&#10;   - ✅ Ahora: `http://localhost:11434/api`&#10;&#10;2. **Sin API Key**:&#10;   - Ollama local NO requiere API key&#10;   - Se eliminó toda la verificación de `OLLAMA_API_KEY`&#10;&#10;3. **Verificación de servicio**:&#10;   - Ahora verifica que Ollama esté corriendo localmente&#10;   - Muestra modelos instalados&#10;&#10;4. **Archivos actualizados**:&#10;   - ✅ `script.py` - Script principal corregido&#10;   - ✅ `test_connection.py` - Test para Ollama local&#10;   - ✅ `execute.bat` - Script de ejecución actualizado&#10;&#10;---&#10;&#10;##  Cómo Usar Ollama (CORRECTO)&#10;&#10;### 1️⃣ Instalar Ollama&#10;&#10;Si no lo tienes instalado:&#10;&#10;**Windows/Mac/Linux:**&#10;```bash&#10;# Descargar e instalar desde:&#10;https://ollama.com/download&#10;```&#10;&#10;### 2️⃣ Verificar que Ollama está corriendo&#10;&#10;**Windows:** Ollama se ejecuta automáticamente como servicio después de instalarlo&#10;&#10;**Si necesitas iniciarlo manualmente:**&#10;```bash&#10;ollama serve&#10;```&#10;&#10;### 3️⃣ Descargar modelos&#10;&#10;Antes de ejecutar el script, necesitas descargar los modelos que quieras usar:&#10;&#10;```bash&#10;# Modelos recomendados para empezar:&#10;ollama pull llama3.2       # Rápido y eficiente (3B)&#10;ollama pull mistral        # Excelente calidad (7B)&#10;ollama pull phi3           # Pequeño pero potente (3.8B)&#10;&#10;# Otros modelos populares:&#10;ollama pull llama3.1:8b    # Alta calidad&#10;ollama pull gemma2:2b      # Muy rápido&#10;ollama pull qwen2.5:7b     # Multilingüe&#10;```&#10;&#10;Ver todos los modelos disponibles: https://ollama.com/library&#10;&#10;### 4️⃣ Verificar instalación&#10;&#10;```bash&#10;cd C:\Users\manel\PyCharmMiscProject\Ollama&#10;python test_connection.py&#10;```&#10;&#10;Deberías ver:&#10;```&#10;✅ Ollama is running!&#10;Found X model(s) installed:&#10;  ✓ llama3.2:latest&#10;  ✓ mistral:latest&#10;...&#10;✅ SUCCESS! Ollama is working correctly!&#10;```&#10;&#10;### 5️⃣ Configurar modelos en script_models_config.py&#10;&#10;Edita el archivo y descomenta los modelos que **ya hayas descargado**:&#10;&#10;```python&#10;MODELS = [&#10;    {&quot;name&quot;: &quot;llama3.2:latest&quot;},    # Si hiciste: ollama pull llama3.2&#10;    {&quot;name&quot;: &quot;mistral:latest&quot;},     # Si hiciste: ollama pull mistral&#10;    # {&quot;name&quot;: &quot;phi3:latest&quot;},      # Descomentar si lo descargaste&#10;]&#10;```&#10;&#10;### 6️⃣ Ejecutar el script&#10;&#10;```bash&#10;execute.bat&#10;```&#10;&#10;O directamente:&#10;```bash&#10;python script.py&#10;```&#10;&#10;---&#10;&#10;##  Verificar modelos instalados&#10;&#10;Para ver qué modelos tienes instalados:&#10;&#10;```bash&#10;ollama list&#10;```&#10;&#10;Salida ejemplo:&#10;```&#10;NAME              ID              SIZE    MODIFIED&#10;llama3.2:latest   a80c4f17acd5    2.0 GB  2 hours ago&#10;mistral:latest    f974a74358d6    4.1 GB  2 hours ago&#10;```&#10;&#10;---&#10;&#10;##  Comandos Útiles de Ollama&#10;&#10;```bash&#10;# Listar modelos instalados&#10;ollama list&#10;&#10;# Descargar un modelo&#10;ollama pull &lt;modelo&gt;&#10;&#10;# Eliminar un modelo&#10;ollama rm &lt;modelo&gt;&#10;&#10;# Probar un modelo interactivamente&#10;ollama run llama3.2&#10;&#10;# Ver información del servicio&#10;ollama serve&#10;```&#10;&#10;---&#10;&#10;## ⚠️ Solución de Problemas&#10;&#10;### Error: &quot;Cannot connect to Ollama&quot;&#10;&#10;**Solución:**&#10;```bash&#10;# Verificar que Ollama está corriendo&#10;ollama serve&#10;&#10;# O en Windows, verificar el servicio&#10;# Buscar &quot;Ollama&quot; en servicios de Windows&#10;```&#10;&#10;### Error: &quot;Model not found&quot;&#10;&#10;**Solución:**&#10;```bash&#10;# Descargar el modelo primero&#10;ollama pull nombre_del_modelo&#10;&#10;# Ejemplo:&#10;ollama pull llama3.2&#10;```&#10;&#10;### Puerto 11434 ya en uso&#10;&#10;**Solución:**&#10;```bash&#10;# Ollama ya está corriendo&#10;# No necesitas hacer nada, solo ejecuta el script&#10;```&#10;&#10;---&#10;&#10;##  Diferencias: Ollama Local vs Cloud APIs&#10;&#10;| Característica | Ollama Local | OpenAI/Anthropic APIs |&#10;|----------------|--------------|----------------------|&#10;| **Requiere internet** | No (solo para descargar) | Sí (siempre) |&#10;| **API Key** | No | Sí |&#10;| **Costo** | Gratis | Pago por uso |&#10;| **Privacidad** | Total (local) | Datos enviados a cloud |&#10;| **Velocidad** | Depende de tu HW | Depende de conexión |&#10;| **Modelos** | Librería Ollama | Modelos propietarios |&#10;&#10;---&#10;&#10;## ✅ Resumen de la Corrección&#10;&#10;**El script ahora funciona correctamente con Ollama Local:**&#10;&#10;1. ✅ No intenta conectarse a URLs inexistentes&#10;2. ✅ Usa la API local de Ollama (localhost:11434)&#10;3. ✅ No requiere API key&#10;4. ✅ Verifica que Ollama esté corriendo&#10;5. ✅ Procesa prompts secuencialmente&#10;6. ✅ Guarda respuestas correctamente&#10;&#10;**Ya está listo para usar** - Solo necesitas:&#10;1. Instalar Ollama&#10;2. Descargar modelos con `ollama pull`&#10;3. Ejecutar el script&#10;&#10;---&#10;&#10;##  Próximos Pasos&#10;&#10;1. **Instala Ollama** desde https://ollama.com/download&#10;2. **Descarga modelos**: `ollama pull llama3.2`&#10;3. **Prueba la conexión**: `python test_connection.py`&#10;4. **Ejecuta el script**: `execute.bat`&#10;&#10;---&#10;&#10;**Fecha de corrección:** 31 de Octubre, 2025&#10;**Archivos corregidos:** script.py, test_connection.py, execute.bat&#10;&#10;" />
              <option name="updatedContent" value="# ✅ CORRECCIÓN APLICADA - Ollama API&#10;&#10;##  URL Correcta Configurada&#10;&#10;Se ha actualizado el script para usar la **API oficial de Ollama**:&#10;&#10;### URL Configurada:&#10;```&#10;https://ollama.com/api&#10;```&#10;&#10;## ✅ Estado Actual&#10;&#10;El script ahora está configurado para conectarse a la API remota de Ollama en **`https://ollama.com/api`**.&#10;&#10;---&#10;&#10;##  Cómo Usar&#10;&#10;### 1️⃣ Verificar Conexión&#10;&#10;```bash&#10;cd C:\Users\manel\PyCharmMiscProject\Ollama&#10;python test_connection.py&#10;```&#10;&#10;### 2️⃣ Configurar Modelos&#10;&#10;Edita `script_models_config.py` y selecciona los modelos que quieras usar:&#10;&#10;```python&#10;MODELS = [&#10;    {&quot;name&quot;: &quot;llama3.2:latest&quot;},&#10;    {&quot;name&quot;: &quot;mistral:latest&quot;},&#10;    # Añade más modelos según necesites&#10;]&#10;```&#10;&#10;Ver modelos disponibles: https://ollama.com/library&#10;&#10;### 3️⃣ Ejecutar el Script&#10;&#10;```bash&#10;execute.bat&#10;```&#10;&#10;O directamente:&#10;```bash&#10;python script.py&#10;```&#10;&#10;---&#10;&#10;## ⚙️ Características&#10;&#10;- ✅ **Procesamiento secuencial**: Un prompt a la vez para mayor estabilidad&#10;- ✅ **Reinicio automático**: Continúa desde donde lo dejó&#10;- ✅ **Guardado incremental**: Guarda cada respuesta inmediatamente  &#10;- ✅ **Logging completo**: Registra todo en `logs/`&#10;- ✅ **Manejo de errores**: Reintentos automáticos&#10;&#10;---&#10;&#10;##  Archivos Actualizados&#10;&#10;- ✅ `script.py` - Actualizado para usar `https://ollama.com/api`&#10;- ✅ `test_connection.py` - Verifica conexión con API&#10;- ✅ `execute.bat` - Verifica conectividad antes de ejecutar&#10;&#10;---&#10;&#10;**Fecha de actualización:** 31 de Octubre, 2025&#10;&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Ollama/execute.bat">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Ollama/execute.bat" />
              <option name="originalContent" value="@echo off&#10;echo ========================================&#10;echo Executing Ollama Local Script&#10;echo ========================================&#10;echo.&#10;&#10;REM Check if Ollama is running&#10;echo Checking if Ollama is running...&#10;curl -s http://localhost:11434/api/tags &gt;nul 2&gt;&amp;1&#10;if errorlevel 1 (&#10;    echo.&#10;    echo ERROR: Ollama is not running!&#10;    echo.&#10;    echo Please start Ollama first:&#10;    echo   ollama serve&#10;    echo.&#10;    echo Or if Ollama is already running in background,&#10;    echo make sure it's accessible at http://localhost:11434&#10;    echo.&#10;    pause&#10;    exit /b 1&#10;)&#10;&#10;echo Ollama is running!&#10;echo.&#10;&#10;REM Activate virtual environment if it exists&#10;if exist .venv\Scripts\activate.bat (&#10;    echo Activating virtual environment...&#10;    call .venv\Scripts\activate.bat&#10;    echo.&#10;)&#10;&#10;echo Starting script execution...&#10;echo.&#10;python script.py&#10;&#10;if errorlevel 1 (&#10;    echo.&#10;    echo ========================================&#10;    echo Script execution failed!&#10;    echo ========================================&#10;    pause&#10;    exit /b 1&#10;) else (&#10;    echo.&#10;    echo ========================================&#10;    echo Script execution completed successfully!&#10;    echo ========================================&#10;)&#10;&#10;pause&#10;&#10;" />
              <option name="updatedContent" value="@echo off&#10;echo ========================================&#10;echo Executing Ollama Cloud API Script&#10;echo ========================================&#10;echo.&#10;&#10;REM Check if API key is set&#10;if &quot;%OLLAMA_API_KEY%&quot;==&quot;&quot; (&#10;    echo ERROR: OLLAMA_API_KEY environment variable is not set!&#10;    echo.&#10;    echo Please set your Ollama API key:&#10;    echo   set OLLAMA_API_KEY=your_api_key_here&#10;    echo.&#10;    echo Or permanently:&#10;    echo   setx OLLAMA_API_KEY &quot;your_api_key_here&quot;&#10;    echo.&#10;    echo Get your API key from: https://ollama.com/&#10;    echo.&#10;    pause&#10;    exit /b 1&#10;)&#10;&#10;echo API Key: [SET]&#10;echo.&#10;&#10;REM Activate virtual environment if it exists&#10;if exist .venv\Scripts\activate.bat (&#10;    echo Activating virtual environment...&#10;    call .venv\Scripts\activate.bat&#10;    echo.&#10;)&#10;&#10;echo Starting script execution...&#10;echo.&#10;python script.py&#10;&#10;if errorlevel 1 (&#10;    echo.&#10;    echo ========================================&#10;    echo Script execution failed!&#10;    echo ========================================&#10;    pause&#10;    exit /b 1&#10;) else (&#10;    echo.&#10;    echo ========================================&#10;    echo Script execution completed successfully!&#10;    echo ========================================&#10;)&#10;&#10;pause&#10;&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/RESUMEN.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/RESUMEN.md" />
              <option name="updatedContent" value="# RESUMEN - Cómo Activar tu GPU&#10;&#10;##  Estado Actual&#10;- ✅ Accelerate instalado&#10;- ❌ PyTorch sin soporte CUDA&#10;- ❌ GPU no detectada&#10;&#10;##  Pasos para Activar GPU&#10;&#10;### 1. Verificar que tienes GPU NVIDIA&#10;Abre CMD o PowerShell y ejecuta:&#10;```bash&#10;nvidia-smi&#10;```&#10;&#10;**Si ves información de tu GPU**: ✅ Continúa al paso 2&#10;**Si ves un error**: ❌ Instala drivers NVIDIA primero&#10;&#10;### 2. Instalar PyTorch con CUDA&#10;Ejecuta el script automático:&#10;```bash&#10;install_pytorch_gpu.bat&#10;```&#10;&#10;**O manualmente:**&#10;```bash&#10;py -m pip uninstall torch torchvision torchaudio -y&#10;py -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118&#10;```&#10;&#10;### 3. Verificar instalación&#10;```bash&#10;py check_gpu.py&#10;```&#10;&#10;Deberías ver:&#10;```&#10;CUDA available: True  ✅&#10;Device name: [Tu GPU]&#10;```&#10;&#10;### 4. Ejecutar tu script&#10;```bash&#10;py script.py&#10;```&#10;&#10;## ⚡ Mejoras Implementadas&#10;&#10;### 1. Detección de GPU mejorada&#10;El script ahora muestra:&#10;- Nombre de la GPU&#10;- Memoria disponible&#10;- Uso de memoria durante la carga&#10;&#10;### 2. Carga de modelos optimizada&#10;- Usa FP16 automáticamente en GPU (ahorra memoria)&#10;- device_map='auto' para mejor distribución&#10;- Libera memoria correctamente entre modelos&#10;&#10;### 3. Logging del prompt actual&#10;Ahora puedes ver:&#10;- test_item siendo procesado&#10;- El prompt completo&#10;- Progreso en tiempo real&#10;&#10;### 4. Optimización de descarga de modelos&#10;- Solo carga el modelo si hay respuestas pendientes&#10;- Elimina el modelo de memoria al terminar&#10;- Limpia caché CUDA entre modelos&#10;&#10;### 5. MAX_WORKERS = 1 (RECOMENDADO)&#10;Para modelos LLM grandes (7B+), usar 1 worker evita:&#10;- Errores &quot;CUDA out of memory&quot;&#10;- Conflictos de memoria&#10;- Crashes del sistema&#10;&#10;##  Velocidad Esperada&#10;&#10;### Con CPU (actual):&#10;- ~1-5 minutos por prompt&#10;- ~2-4 horas para 50 prompts&#10;&#10;### Con GPU (después de instalar):&#10;- ~5-15 segundos por prompt&#10;- ~5-15 minutos para 50 prompts&#10;&#10;** Hasta 20x más rápido!**&#10;&#10;## ⚙️ Configuración de MAX_WORKERS&#10;&#10;| GPU Memory | Modelos Pequeños | Modelos 7B+ |&#10;|-----------|------------------|-------------|&#10;| 8GB       | 1-2              | 1           |&#10;| 12GB      | 2-3              | 1           |&#10;| 16GB      | 3-4              | 1-2         |&#10;| 24GB+     | 4-6              | 2-4         |&#10;&#10;**Recomendación actual: MAX_WORKERS = 1**&#10;&#10;##  Solución de Problemas&#10;&#10;### &quot;No GPU detected&quot;&#10;```bash&#10;# 1. Verifica drivers&#10;nvidia-smi&#10;&#10;# 2. Reinstala PyTorch con CUDA&#10;install_pytorch_gpu.bat&#10;&#10;# 3. Reinicia terminal/IDE&#10;```&#10;&#10;### &quot;CUDA out of memory&quot;&#10;```python&#10;# En script.py, línea ~62:&#10;MAX_WORKERS = 1  # Reduce si es necesario&#10;```&#10;&#10;### &quot;Loading an FP8 quantized model requires accelerate&quot;&#10;```bash&#10;py -m pip install accelerate&#10;```&#10;✅ Ya instalado!&#10;&#10;##  Archivos Creados&#10;&#10;1. `install_pytorch_gpu.bat` - Script de instalación automática&#10;2. `check_gpu.py` - Verificar estado de GPU&#10;3. `GPU_SETUP.md` - Guía completa&#10;4. `RESUMEN.md` - Este archivo&#10;&#10;## ✅ Siguiente Paso&#10;&#10;**Ejecuta ahora:**&#10;```bash&#10;install_pytorch_gpu.bat&#10;```&#10;&#10;Esto instalará PyTorch con soporte CUDA y verificará que tu GPU esté funcionando.&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/install_pytorch_gpu.bat">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/install_pytorch_gpu.bat" />
              <option name="originalContent" value="&#10;" />
              <option name="updatedContent" value="@echo off&#10;echo ========================================&#10;echo Instalando PyTorch con soporte para GPU&#10;echo ========================================&#10;echo.&#10;&#10;echo Verificando GPU NVIDIA...&#10;nvidia-smi&#10;if errorlevel 1 (&#10;    echo.&#10;    echo [ERROR] No se detecta GPU NVIDIA o drivers no instalados&#10;    echo Por favor instala los drivers NVIDIA mas recientes&#10;    echo https://www.nvidia.com/Download/index.aspx&#10;    pause&#10;    exit /b 1&#10;)&#10;&#10;echo.&#10;echo Desinstalando PyTorch actual...&#10;py -m pip uninstall torch torchvision torchaudio -y&#10;&#10;echo.&#10;echo Instalando PyTorch con CUDA 11.8...&#10;echo (Compatible con la mayoria de GPUs NVIDIA)&#10;py -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118&#10;&#10;echo.&#10;echo Instalando librerias adicionales...&#10;py -m pip install accelerate&#10;&#10;echo.&#10;echo ========================================&#10;echo Instalacion completa!&#10;echo ========================================&#10;echo.&#10;echo Verificando instalacion...&#10;py check_gpu.py&#10;&#10;echo.&#10;echo Si ves &quot;CUDA available: True&quot;, tu GPU esta lista para usar!&#10;echo.&#10;pause&#10;&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/models_config.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/models_config.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Configuration file for models used in prompt processing.&#10;Each model has its own configuration including whether to use QA pipeline&#10;and whether to trust remote code.&#10;&quot;&quot;&quot;&#10;&#10;MODELS = [&#10;    {&quot;name&quot;: &quot;distilbert-base-cased-distilled-squad&quot;, &quot;use_qa_pipeline&quot;: True, &quot;trust_remote_code&quot;: False},&#10;    {&quot;name&quot;: &quot;mistralai/Mistral-7B-Instruct-v0.2&quot;, &quot;use_qa_pipeline&quot;: False, &quot;trust_remote_code&quot;: False},&#10;    {&quot;name&quot;: &quot;microsoft/phi-4&quot;, &quot;use_qa_pipeline&quot;: False, &quot;trust_remote_code&quot;: False},&#10;    {&quot;name&quot;: &quot;tiiuae/falcon-7b-instruct&quot;, &quot;use_qa_pipeline&quot;: False, &quot;trust_remote_code&quot;: False},  # Falcon is already integrated in transformers&#10;    {&quot;name&quot;: &quot;arcee-ai/Arcee-Blitz&quot;, &quot;use_qa_pipeline&quot;: False, &quot;trust_remote_code&quot;: False},&#10;    {&quot;name&quot;: &quot;arcee-ai/Virtuoso-Lite&quot;, &quot;use_qa_pipeline&quot;: False, &quot;trust_remote_code&quot;: False},&#10;    {&quot;name&quot;: &quot;inclusionAI/Ling-1T-FP8&quot;, &quot;use_qa_pipeline&quot;: False, &quot;trust_remote_code&quot;: True},&#10;    {&quot;name&quot;: &quot;moonshotai/Kimi-K2-Instruct-0905&quot;, &quot;use_qa_pipeline&quot;: False, &quot;trust_remote_code&quot;: True},&#10;&#10;    # Models that require authentication:&#10;    {&quot;name&quot;: &quot;google/gemma-3-1b-it&quot;, &quot;use_qa_pipeline&quot;: False, &quot;trust_remote_code&quot;: False},  # Requires gated access&#10;]&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>