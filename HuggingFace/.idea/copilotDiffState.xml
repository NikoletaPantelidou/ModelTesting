<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/GPU_SETUP.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/GPU_SETUP.md" />
              <option name="updatedContent" value="# Guía para Configurar GPU con PyTorch&#10;&#10;## Problema&#10;Tu PyTorch está instalado sin soporte CUDA, por lo que no detecta tu GPU.&#10;&#10;## Solución&#10;&#10;### Paso 1: Verificar tu GPU&#10;Ejecuta en PowerShell o CMD:&#10;```bash&#10;nvidia-smi&#10;```&#10;Esto te mostrará tu GPU NVIDIA y la versión de CUDA Driver instalada.&#10;&#10;### Paso 2: Instalar PyTorch con CUDA&#10;&#10;**Opción A - Ejecutar el script automático (RECOMENDADO):**&#10;```bash&#10;install_pytorch_gpu.bat&#10;```&#10;&#10;**Opción B - Instalación manual:**&#10;&#10;Para CUDA 11.8 (compatible con la mayoría de GPUs):&#10;```bash&#10;pip uninstall torch torchvision torchaudio -y&#10;pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118&#10;```&#10;&#10;Para CUDA 12.1 (GPUs más recientes):&#10;```bash&#10;pip uninstall torch torchvision torchaudio -y&#10;pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121&#10;```&#10;&#10;### Paso 3: Verificar la instalación&#10;```bash&#10;py check_gpu.py&#10;```&#10;&#10;Deberías ver:&#10;```&#10;CUDA available: True&#10;CUDA version: 11.8 (o 12.1)&#10;Device count: 1&#10;Device name: [Nombre de tu GPU]&#10;```&#10;&#10;### Paso 4: Ejecutar el script&#10;```bash&#10;py script.py&#10;```&#10;&#10;## Configuración de MAX_WORKERS&#10;&#10;El script está configurado con `MAX_WORKERS = 1` por defecto. Esto es **RECOMENDADO** para:&#10;- Modelos grandes (7B+ parámetros)&#10;- GPUs con menos de 24GB de memoria&#10;&#10;### ¿Cuándo aumentar MAX_WORKERS?&#10;&#10;Solo aumenta si:&#10;- Tienes una GPU con mucha memoria (24GB+)&#10;- Estás usando modelos pequeños (&lt;3B parámetros)&#10;- No ves errores &quot;CUDA out of memory&quot;&#10;&#10;**Valores recomendados:**&#10;- GPU 8GB (RTX 3060, etc.): `MAX_WORKERS = 1`&#10;- GPU 12GB (RTX 3080, etc.): `MAX_WORKERS = 1-2`&#10;- GPU 16GB (RTX 4080, etc.): `MAX_WORKERS = 2`&#10;- GPU 24GB+ (RTX 4090, A100, etc.): `MAX_WORKERS = 2-4`&#10;&#10;## Beneficios de usar GPU&#10;&#10;**Sin GPU (CPU):**&#10;- Velocidad: ~1-5 minutos por prompt&#10;- Tiempo total: ~2-4 horas para 50 prompts&#10;&#10;**Con GPU:**&#10;- Velocidad: ~5-15 segundos por prompt&#10;- Tiempo total: ~5-15 minutos para 50 prompts&#10;&#10;**¡Hasta 20x más rápido!**&#10;&#10;## Problemas comunes&#10;&#10;### &quot;CUDA out of memory&quot;&#10;- Reduce MAX_WORKERS a 1&#10;- Cierra otras aplicaciones que usen GPU&#10;- Usa modelos más pequeños&#10;&#10;### &quot;No GPU detected&quot; después de instalar&#10;- Verifica que tienes drivers NVIDIA actualizados&#10;- Reinicia el terminal/IDE&#10;- Verifica la versión de CUDA instalada con `nvidia-smi`&#10;&#10;### Modelo requiere FP8 quantization&#10;- Instala accelerate: `pip install accelerate`&#10;- Algunos modelos requieren GPUs específicas (H100, etc.)&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/RESUMEN.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/RESUMEN.md" />
              <option name="updatedContent" value="# RESUMEN - Cómo Activar tu GPU&#10;&#10;##  Estado Actual&#10;- ✅ Accelerate instalado&#10;- ❌ PyTorch sin soporte CUDA&#10;- ❌ GPU no detectada&#10;&#10;##  Pasos para Activar GPU&#10;&#10;### 1. Verificar que tienes GPU NVIDIA&#10;Abre CMD o PowerShell y ejecuta:&#10;```bash&#10;nvidia-smi&#10;```&#10;&#10;**Si ves información de tu GPU**: ✅ Continúa al paso 2&#10;**Si ves un error**: ❌ Instala drivers NVIDIA primero&#10;&#10;### 2. Instalar PyTorch con CUDA&#10;Ejecuta el script automático:&#10;```bash&#10;install_pytorch_gpu.bat&#10;```&#10;&#10;**O manualmente:**&#10;```bash&#10;py -m pip uninstall torch torchvision torchaudio -y&#10;py -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118&#10;```&#10;&#10;### 3. Verificar instalación&#10;```bash&#10;py check_gpu.py&#10;```&#10;&#10;Deberías ver:&#10;```&#10;CUDA available: True  ✅&#10;Device name: [Tu GPU]&#10;```&#10;&#10;### 4. Ejecutar tu script&#10;```bash&#10;py script.py&#10;```&#10;&#10;## ⚡ Mejoras Implementadas&#10;&#10;### 1. Detección de GPU mejorada&#10;El script ahora muestra:&#10;- Nombre de la GPU&#10;- Memoria disponible&#10;- Uso de memoria durante la carga&#10;&#10;### 2. Carga de modelos optimizada&#10;- Usa FP16 automáticamente en GPU (ahorra memoria)&#10;- device_map='auto' para mejor distribución&#10;- Libera memoria correctamente entre modelos&#10;&#10;### 3. Logging del prompt actual&#10;Ahora puedes ver:&#10;- test_item siendo procesado&#10;- El prompt completo&#10;- Progreso en tiempo real&#10;&#10;### 4. Optimización de descarga de modelos&#10;- Solo carga el modelo si hay respuestas pendientes&#10;- Elimina el modelo de memoria al terminar&#10;- Limpia caché CUDA entre modelos&#10;&#10;### 5. MAX_WORKERS = 1 (RECOMENDADO)&#10;Para modelos LLM grandes (7B+), usar 1 worker evita:&#10;- Errores &quot;CUDA out of memory&quot;&#10;- Conflictos de memoria&#10;- Crashes del sistema&#10;&#10;##  Velocidad Esperada&#10;&#10;### Con CPU (actual):&#10;- ~1-5 minutos por prompt&#10;- ~2-4 horas para 50 prompts&#10;&#10;### Con GPU (después de instalar):&#10;- ~5-15 segundos por prompt&#10;- ~5-15 minutos para 50 prompts&#10;&#10;** Hasta 20x más rápido!**&#10;&#10;## ⚙️ Configuración de MAX_WORKERS&#10;&#10;| GPU Memory | Modelos Pequeños | Modelos 7B+ |&#10;|-----------|------------------|-------------|&#10;| 8GB       | 1-2              | 1           |&#10;| 12GB      | 2-3              | 1           |&#10;| 16GB      | 3-4              | 1-2         |&#10;| 24GB+     | 4-6              | 2-4         |&#10;&#10;**Recomendación actual: MAX_WORKERS = 1**&#10;&#10;##  Solución de Problemas&#10;&#10;### &quot;No GPU detected&quot;&#10;```bash&#10;# 1. Verifica drivers&#10;nvidia-smi&#10;&#10;# 2. Reinstala PyTorch con CUDA&#10;install_pytorch_gpu.bat&#10;&#10;# 3. Reinicia terminal/IDE&#10;```&#10;&#10;### &quot;CUDA out of memory&quot;&#10;```python&#10;# En script.py, línea ~62:&#10;MAX_WORKERS = 1  # Reduce si es necesario&#10;```&#10;&#10;### &quot;Loading an FP8 quantized model requires accelerate&quot;&#10;```bash&#10;py -m pip install accelerate&#10;```&#10;✅ Ya instalado!&#10;&#10;##  Archivos Creados&#10;&#10;1. `install_pytorch_gpu.bat` - Script de instalación automática&#10;2. `check_gpu.py` - Verificar estado de GPU&#10;3. `GPU_SETUP.md` - Guía completa&#10;4. `RESUMEN.md` - Este archivo&#10;&#10;## ✅ Siguiente Paso&#10;&#10;**Ejecuta ahora:**&#10;```bash&#10;install_pytorch_gpu.bat&#10;```&#10;&#10;Esto instalará PyTorch con soporte CUDA y verificará que tu GPU esté funcionando.&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/install_pytorch_gpu.bat">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/install_pytorch_gpu.bat" />
              <option name="originalContent" value="&#10;" />
              <option name="updatedContent" value="@echo off&#10;echo ========================================&#10;echo Instalando PyTorch con soporte para GPU&#10;echo ========================================&#10;echo.&#10;&#10;echo Verificando GPU NVIDIA...&#10;nvidia-smi&#10;if errorlevel 1 (&#10;    echo.&#10;    echo [ERROR] No se detecta GPU NVIDIA o drivers no instalados&#10;    echo Por favor instala los drivers NVIDIA mas recientes&#10;    echo https://www.nvidia.com/Download/index.aspx&#10;    pause&#10;    exit /b 1&#10;)&#10;&#10;echo.&#10;echo Desinstalando PyTorch actual...&#10;py -m pip uninstall torch torchvision torchaudio -y&#10;&#10;echo.&#10;echo Instalando PyTorch con CUDA 11.8...&#10;echo (Compatible con la mayoria de GPUs NVIDIA)&#10;py -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118&#10;&#10;echo.&#10;echo Instalando librerias adicionales...&#10;py -m pip install accelerate&#10;&#10;echo.&#10;echo ========================================&#10;echo Instalacion completa!&#10;echo ========================================&#10;echo.&#10;echo Verificando instalacion...&#10;py check_gpu.py&#10;&#10;echo.&#10;echo Si ves &quot;CUDA available: True&quot;, tu GPU esta lista para usar!&#10;echo.&#10;pause&#10;&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/models_config.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/models_config.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Configuration file for models used in prompt processing.&#10;Each model has its own configuration including whether to use QA pipeline&#10;and whether to trust remote code.&#10;&quot;&quot;&quot;&#10;&#10;MODELS = [&#10;    {&quot;name&quot;: &quot;distilbert-base-cased-distilled-squad&quot;, &quot;use_qa_pipeline&quot;: True, &quot;trust_remote_code&quot;: False},&#10;    {&quot;name&quot;: &quot;mistralai/Mistral-7B-Instruct-v0.2&quot;, &quot;use_qa_pipeline&quot;: False, &quot;trust_remote_code&quot;: False},&#10;    {&quot;name&quot;: &quot;microsoft/phi-4&quot;, &quot;use_qa_pipeline&quot;: False, &quot;trust_remote_code&quot;: False},&#10;    {&quot;name&quot;: &quot;tiiuae/falcon-7b-instruct&quot;, &quot;use_qa_pipeline&quot;: False, &quot;trust_remote_code&quot;: False},  # Falcon is already integrated in transformers&#10;    {&quot;name&quot;: &quot;arcee-ai/Arcee-Blitz&quot;, &quot;use_qa_pipeline&quot;: False, &quot;trust_remote_code&quot;: False},&#10;    {&quot;name&quot;: &quot;arcee-ai/Virtuoso-Lite&quot;, &quot;use_qa_pipeline&quot;: False, &quot;trust_remote_code&quot;: False},&#10;    {&quot;name&quot;: &quot;inclusionAI/Ling-1T-FP8&quot;, &quot;use_qa_pipeline&quot;: False, &quot;trust_remote_code&quot;: True},&#10;    {&quot;name&quot;: &quot;moonshotai/Kimi-K2-Instruct-0905&quot;, &quot;use_qa_pipeline&quot;: False, &quot;trust_remote_code&quot;: True},&#10;&#10;    # Models that require authentication:&#10;    {&quot;name&quot;: &quot;google/gemma-3-1b-it&quot;, &quot;use_qa_pipeline&quot;: False, &quot;trust_remote_code&quot;: False},  # Requires gated access&#10;]&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>