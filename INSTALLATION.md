# üöÄ Gu√≠a de Instalaci√≥n Completa

## Opci√≥n 1: Ollama Cloud (Recomendado para Empezar)

### ‚úÖ Ventajas
- ‚ö° Setup en 5 minutos
- üíª No requiere GPU
- üéØ Sin problemas t√©cnicos

### üìã Requisitos
- Python 3.8+
- Conexi√≥n a Internet
- API Key de Ollama Cloud

### üîß Pasos de Instalaci√≥n

#### 1. Navegar a la carpeta
```bash
cd C:\Users\manel\PyCharmMiscProject\Ollama
```

#### 2. Ejecutar setup autom√°tico
```bash
setup.bat
```

Esto har√°:
- Crear entorno virtual
- Instalar pandas y requests
- Mostrar instrucciones para API key

#### 3. Obtener API Key
1. Visita: https://ollama.com/cloud
2. Crea cuenta/inicia sesi√≥n
3. Genera tu API key

#### 4. Configurar API Key

**Opci√≥n A - Temporal (actual sesi√≥n):**
```bash
set OLLAMA_API_KEY=tu_api_key_aqui
```

**Opci√≥n B - Permanente:**
```bash
setx OLLAMA_API_KEY "tu_api_key_aqui"
```

#### 5. Verificar instalaci√≥n
```bash
python test_connection.py
```

Deber√≠as ver:
```
‚úÖ SUCCESS! API is working correctly!
‚úì Connection test passed!
```

#### 6. ¬°Ejecutar!
```bash
execute.bat
```

---

## Opci√≥n 2: HuggingFace Local (Para Usuarios Avanzados)

### ‚úÖ Ventajas
- üîí Datos permanecen locales
- üí∞ Sin costos de API (despu√©s de inversi√≥n inicial)
- üéõÔ∏è Control total

### üìã Requisitos M√≠nimos
- Python 3.8+
- GPU NVIDIA con 8GB+ VRAM (recomendado)
- CUDA 11.8+ instalado
- 16GB+ RAM
- 50GB+ espacio libre en disco

### üìã Requisitos Recomendados
- GPU NVIDIA con 12GB+ VRAM (ej: RTX 3060 12GB, RTX 4070, A4000)
- CUDA 12.1+
- 32GB RAM
- 100GB+ espacio libre (SSD)

### üîß Pasos de Instalaci√≥n

#### 1. Verificar/Instalar NVIDIA Drivers

**Verificar instalaci√≥n:**
```bash
nvidia-smi
```

Deber√≠as ver informaci√≥n de tu GPU.

**Si no funciona:**
1. Descarga drivers desde: https://www.nvidia.com/Download/index.aspx
2. Selecciona tu GPU
3. Instala los drivers
4. Reinicia el PC

#### 2. Instalar CUDA Toolkit

**Verificar si ya est√° instalado:**
```bash
nvcc --version
```

**Si no est√° instalado:**
1. Descarga CUDA desde: https://developer.nvidia.com/cuda-downloads
2. Selecciona tu sistema operativo
3. Instala CUDA Toolkit
4. Reinicia el PC

#### 3. Navegar a la carpeta
```bash
cd C:\Users\manel\PyCharmMiscProject\huggingFace
```

#### 4. Crear entorno virtual
```bash
python -m venv .venv
.venv\Scripts\activate
```

#### 5. Actualizar pip
```bash
python -m pip install --upgrade pip
```

#### 6. Instalar PyTorch con CUDA

**Para CUDA 12.1:**
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

**Para CUDA 11.8:**
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

**Para CPU solamente (no recomendado):**
```bash
pip install torch torchvision torchaudio
```

#### 7. Instalar otras dependencias
```bash
pip install transformers pandas huggingface_hub accelerate
```

#### 8. Verificar instalaci√≥n GPU
```bash
python check_gpu.py
```

Deber√≠as ver:
```
CUDA available: True
CUDA device count: 1
Current CUDA device: 0
CUDA device name: NVIDIA GeForce RTX 3060
```

#### 9. Configurar HuggingFace Token (opcional)

Algunos modelos requieren autenticaci√≥n:

1. Visita: https://huggingface.co/settings/tokens
2. Crea un token (Read access)
3. En `script.py`, actualiza l√≠nea 19:
```python
HF_TOKEN = "tu_token_aqui"
```

#### 10. ¬°Ejecutar!
```bash
execute.bat
```

---

## ‚ö†Ô∏è Soluci√≥n de Problemas

### HuggingFace Local

#### Problema: "CUDA not available"

**Soluci√≥n 1:** Reinstalar PyTorch con CUDA
```bash
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

**Soluci√≥n 2:** Verificar drivers NVIDIA
```bash
nvidia-smi
```

**Soluci√≥n 3:** Verificar variable CUDA_PATH
```bash
echo %CUDA_PATH%
```
Deber√≠a mostrar: `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.1`

#### Problema: "CUDA out of memory"

**Soluci√≥n 1:** Reducir batch size / workers
En `script.py`:
```python
MAX_WORKERS = 1  # Reducir a 1
```

**Soluci√≥n 2:** Usar modelos m√°s peque√±os
En `script_models_config.py`, comenta modelos grandes:
```python
# {"name": "microsoft/phi-4", ...},  # Comentar modelos grandes
```

**Soluci√≥n 3:** Usar FP16 (ya configurado)
Ya est√° usando `torch.float16` para GPU.

#### Problema: "Model download too slow"

**Soluci√≥n:** Usar proxy HF o mirror:
```bash
set HF_ENDPOINT=https://hf-mirror.com
```

### Ollama Cloud

#### Problema: "OLLAMA_API_KEY not set"

**Soluci√≥n:**
```bash
set OLLAMA_API_KEY=tu_api_key_aqui
```

Para hacerlo permanente:
```bash
setx OLLAMA_API_KEY "tu_api_key_aqui"
```
Luego reinicia el cmd/PowerShell.

#### Problema: "Invalid API key"

**Soluci√≥n:**
1. Verifica que la key est√© correcta
2. Verifica que la cuenta est√© activa
3. Genera una nueva key en https://ollama.com/cloud

#### Problema: "Connection timeout"

**Soluci√≥n 1:** Verificar internet
```bash
ping api.ollama.com
```

**Soluci√≥n 2:** Reducir workers
En `script.py`:
```python
MAX_WORKERS = 1
```

**Soluci√≥n 3:** Aumentar timeout
En `script.py`, l√≠nea 95:
```python
response = requests.post(url, json=payload, headers=headers, timeout=120)  # Aumentar a 120
```

#### Problema: "Rate limit exceeded"

**Soluci√≥n:** Reducir paralelismo
```python
MAX_WORKERS = 1
```

Y a√±adir delay entre requests en `script.py`, funci√≥n `call_ollama_api`:
```python
time.sleep(1)  # Esperar 1 segundo entre llamadas
```

---

## üß™ Verificar Instalaci√≥n

### HuggingFace
```bash
cd huggingFace
python check_gpu.py
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
python -c "import transformers; print(f'Transformers: {transformers.__version__}')"
```

### Ollama
```bash
cd Ollama
python test_connection.py
python -c "import requests; print(f'Requests: {requests.__version__}')"
```

---

## üìä Comparaci√≥n de Setup

| Aspecto | HuggingFace | Ollama |
|---------|-------------|---------|
| Tiempo de instalaci√≥n | 1-2 horas | 5-10 minutos |
| Dificultad | Alta | Baja |
| Requisitos de hardware | GPU potente | Cualquier PC |
| Tama√±o descarga inicial | 5-10GB (PyTorch + CUDA) | < 100MB |
| Requiere conocimientos t√©cnicos | S√≠ (CUDA, drivers) | No |

---

## üéØ ¬øCu√°l Instalar Primero?

### Para Principiantes
**‚Üí Ollama Cloud**
- R√°pido de configurar
- Sin problemas t√©cnicos
- Puedes probar el sistema inmediatamente

### Para Usuarios Avanzados
**‚Üí Ambos**
1. Instala Ollama primero (para probar)
2. Luego configura HuggingFace (para producci√≥n)

### Para Producci√≥n
**Depende de tu caso:**
- **Alto volumen + datos sensibles** ‚Üí HuggingFace
- **Bajo volumen + necesitas flexibilidad** ‚Üí Ollama
- **Ambos casos** ‚Üí Instala ambos y elige seg√∫n necesidad

---

## üìö Pr√≥ximos Pasos

Despu√©s de la instalaci√≥n:

1. ‚úÖ Lee el quickstart correspondiente:
   - Ollama: `Ollama/QUICKSTART.md`
   - HuggingFace: Similar a Ollama pero sin API key

2. ‚úÖ Ejecuta con datos de ejemplo

3. ‚úÖ Revisa resultados en `answers/`

4. ‚úÖ Lee `COMPARISON.md` para entender diferencias

5. ‚úÖ Ajusta configuraci√≥n seg√∫n tus necesidades

---

## üÜò ¬øNecesitas Ayuda?

### Documentaci√≥n
- `PROJECT_STRUCTURE.md` - Estructura completa
- `COMPARISON.md` - Comparaci√≥n detallada
- `Ollama/README.md` - Docs de Ollama
- `Ollama/QUICKSTART.md` - Inicio r√°pido

### Recursos Online
- **PyTorch + CUDA**: https://pytorch.org/get-started/locally/
- **Ollama Cloud**: https://docs.ollama.com/cloud
- **HuggingFace**: https://huggingface.co/docs

### Verificaci√≥n R√°pida

```bash
# HuggingFace
python check_gpu.py

# Ollama
python test_connection.py
```

---

¬°Buena suerte con la instalaci√≥n! üöÄ

