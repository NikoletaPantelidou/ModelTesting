# Estructura del Proyecto

```
PyCharmMiscProject/
â”‚
â”œâ”€â”€ COMPARISON.md                      # ComparaciÃ³n HuggingFace vs Ollama
â”‚
â”œâ”€â”€ huggingFace/                       # ğŸ–¥ï¸ IMPLEMENTACIÃ“N LOCAL
â”‚   â”œâ”€â”€ script.py                      # Script principal (GPU)
â”‚   â”œâ”€â”€ script_models_config.py        # Config de modelos HF
â”‚   â”œâ”€â”€ execute.bat                    # Ejecutar script
â”‚   â”œâ”€â”€ clean_cache.bat               # Limpiar cachÃ©
â”‚   â”œâ”€â”€ check_gpu.py                  # Verificar GPU
â”‚   â”œâ”€â”€ requirements.txt              # torch, transformers, etc.
â”‚   â”‚
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â””â”€â”€ example.csv               # Entrada de prompts
â”‚   â”‚
â”‚   â”œâ”€â”€ answers/
â”‚   â”‚   â”œâ”€â”€ distilbert-*_answers.csv
â”‚   â”‚   â”œâ”€â”€ mistral-*_answers.csv
â”‚   â”‚   â””â”€â”€ ...                       # Respuestas por modelo
â”‚   â”‚
â”‚   â””â”€â”€ logs/
â”‚       â””â”€â”€ execution_*.log           # Logs de ejecuciÃ³n
â”‚
â””â”€â”€ Ollama/                           # â˜ï¸ IMPLEMENTACIÃ“N CLOUD
    â”œâ”€â”€ script.py                     # Script principal (API)
    â”œâ”€â”€ script_models_config.py       # Config de modelos Ollama
    â”œâ”€â”€ test_connection.py            # Test de conexiÃ³n API
    â”œâ”€â”€ execute.bat                   # Ejecutar script
    â”œâ”€â”€ setup.bat                     # ConfiguraciÃ³n inicial
    â”œâ”€â”€ clean_cache.bat              # Limpiar cachÃ©
    â”œâ”€â”€ requirements.txt             # requests, pandas
    â”œâ”€â”€ README.md                    # DocumentaciÃ³n completa
    â”œâ”€â”€ QUICKSTART.md                # Inicio rÃ¡pido
    â”œâ”€â”€ .gitignore                   # Archivos a ignorar
    â”‚
    â”œâ”€â”€ prompts/
    â”‚   â””â”€â”€ example.csv              # Entrada de prompts
    â”‚
    â”œâ”€â”€ answers/
    â”‚   â”œâ”€â”€ llama3.2-latest_answers.csv
    â”‚   â”œâ”€â”€ mistral-latest_answers.csv
    â”‚   â””â”€â”€ ...                      # Respuestas por modelo
    â”‚
    â””â”€â”€ logs/
        â””â”€â”€ execution_*.log          # Logs de ejecuciÃ³n
```

## ğŸ“ DescripciÃ³n de Archivos

### Archivos Principales

| Archivo | DescripciÃ³n | UbicaciÃ³n |
|---------|-------------|-----------|
| `script.py` | Motor principal de procesamiento | HF y Ollama |
| `script_models_config.py` | ConfiguraciÃ³n de modelos a usar | HF y Ollama |
| `execute.bat` | Script de ejecuciÃ³n rÃ¡pida | HF y Ollama |
| `requirements.txt` | Dependencias Python | HF y Ollama |

### Archivos de ConfiguraciÃ³n

| Archivo | DescripciÃ³n | UbicaciÃ³n |
|---------|-------------|-----------|
| `setup.bat` | Setup inicial + instalaciÃ³n | Solo Ollama |
| `check_gpu.py` | Verificar GPU disponible | Solo HF |
| `test_connection.py` | Test API connection | Solo Ollama |
| `clean_cache.bat` | Limpiar archivos cache | HF y Ollama |

### DocumentaciÃ³n

| Archivo | DescripciÃ³n | UbicaciÃ³n |
|---------|-------------|-----------|
| `README.md` | DocumentaciÃ³n completa | Solo Ollama |
| `QUICKSTART.md` | GuÃ­a de inicio rÃ¡pido | Solo Ollama |
| `COMPARISON.md` | ComparaciÃ³n HF vs Ollama | RaÃ­z |

### Directorios de Datos

| Directorio | Contenido | PropÃ³sito |
|------------|-----------|-----------|
| `prompts/` | Archivos CSV con prompts | Entrada del sistema |
| `answers/` | Archivos CSV con respuestas | Salida del sistema |
| `logs/` | Logs de ejecuciÃ³n | Debug y monitoreo |
| `__pycache__/` | Cache de Python | Temporal (auto-generado) |
| `.venv/` | Entorno virtual | Aislamiento de deps |

## ğŸ”„ Flujo de Datos

```
prompts/example.csv
        â†“
  [script.py]
   â”œâ”€ Model 1 â†’ answers/model1_answers.csv
   â”œâ”€ Model 2 â†’ answers/model2_answers.csv
   â””â”€ Model N â†’ answers/modelN_answers.csv
        â†“
   logs/execution_[timestamp].log
```

## ğŸ“Š Formato de Archivos

### Input (prompts/example.csv)
```csv
prompt;test_item
"Context text, question?";test_1
"Another context, question?";test_2
```

### Output (answers/model_answers.csv)
```csv
prompt;test_item;answer
"Context text, question?";test_1;"The answer is..."
"Another context, question?";test_2;"Another answer..."
```

### Logs (logs/execution_*.log)
```
2025-10-31 19:27:00 - INFO - [INFO] Loading model: llama3.2:latest
2025-10-31 19:27:05 - INFO - [INFO] Processing row 1/10
2025-10-31 19:27:10 - INFO - [OK] Row 1 completed
```

## ğŸš€ Comandos RÃ¡pidos

### HuggingFace (Local)
```bash
cd huggingFace
execute.bat
```

### Ollama (Cloud)
```bash
cd Ollama
set OLLAMA_API_KEY=your_key_here
execute.bat
```

## ğŸ“ Variables de Entorno

### HuggingFace
- `HF_TOKEN` - Token de HuggingFace (en script.py)

### Ollama
- `OLLAMA_API_KEY` - API key de Ollama Cloud (requerida)

## ğŸ”§ Archivos de ConfiguraciÃ³n Clave

### HuggingFace: script.py (lÃ­neas 50-56)
```python
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
TEMPERATURE = 0.0
OUTPUT_DIR = "answers"
MAX_WORKERS = 4
```

### Ollama: script.py (lÃ­neas 21-29)
```python
OLLAMA_API_KEY = os.getenv("OLLAMA_API_KEY", "")
OLLAMA_API_BASE = "https://api.ollama.com/v1"
MAX_WORKERS = 2
TEMPERATURE = 0.0
MAX_TOKENS = 300
```

## ğŸ“¦ Dependencias

### HuggingFace
- torch (PyTorch con CUDA)
- transformers
- pandas
- huggingface_hub

### Ollama
- requests
- pandas

## ğŸ¯ CuÃ¡ndo Usar Cada Carpeta

| Escenario | Usa | RazÃ³n |
|-----------|-----|-------|
| Tengo GPU potente | `huggingFace/` | Mejor rendimiento local |
| No tengo GPU | `Ollama/` | No requiere GPU |
| Datos sensibles | `huggingFace/` | Todo queda local |
| Prototipo rÃ¡pido | `Ollama/` | Setup en minutos |
| Alto volumen | `huggingFace/` | Sin costos de API |
| Bajo volumen | `Ollama/` | Pago por uso |

## ğŸ” .gitignore

Ambas carpetas ignoran:
- `__pycache__/`
- `.venv/`
- `logs/` (opcionales)
- `answers/` (opcional)

## ğŸ“Œ Notas Importantes

1. **Los archivos CSV son compatibles** entre HF y Ollama
2. **Los modelos NO son intercambiables** (diferente naming)
3. **Ambos soportan paralelizaciÃ³n** con ThreadPoolExecutor
4. **Ambos soportan reinicio automÃ¡tico** (skip completados)
5. **Los logs tienen el mismo formato** para facilitar debugging

## ğŸ†˜ Ayuda RÃ¡pida

```bash
# HuggingFace
cd huggingFace
python check_gpu.py          # Verificar GPU

# Ollama
cd Ollama
python test_connection.py   # Verificar API
python script.py --help     # Ver opciones
```

## ğŸ“š MÃ¡s InformaciÃ³n

- Ver `Ollama/README.md` para detalles de Ollama Cloud
- Ver `Ollama/QUICKSTART.md` para inicio rÃ¡pido
- Ver `COMPARISON.md` para comparaciÃ³n detallada

