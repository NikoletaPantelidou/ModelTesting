# ğŸ§  Multi-LLM Linguistic Test Framework

This project provides a **testing pipeline** for collecting responses from multiple large language models (LLMs) using a set of linguistic prompts.  
This project provides automated testing to measure each modelâ€™s linguistic reasoning capabilities.

---

## ğŸ“š Table of Contents
- [About](#about)
- [Features](#features)
- [Project Structure](#project-structure)
- [Tech Stack](#tech-stack)
- [Setup & Installation](#setup--installation)
- [License](#license)

---

## ğŸ’¡ About

This repository contains scripts and utilities for **benchmarking multiple LLMs** on a controlled **linguistic evaluation task**.  
It systematically measures accuracy, consistency, and reasoning quality across different model families (OpenAI, Anthropic, Mistral, etc.).

The main objectives are to:
- Evaluate LLMsâ€™ performance on fine-grained linguistic phenomena  
- Compare how different kinds of models reason and comprehend language in more complex prompts

---

## âœ¨ Features
- ğŸ§© Tests **25 Large Language Models (LLMs)**  
- ğŸ” Benchmarks **linguistic comprehension and reasoning**   
- ğŸ§  Supports custom prompt templates and datasets  

---

## ğŸ“ Project Structure

```bash
llm-linguistic-eval/
â”œâ”€â”€ script.py            # Main Python script that runs 25 LLMs 
â”œâ”€â”€ prompts.csv          # Input file containing the linguistic prompts 
â”œâ”€â”€ answers.csv          # Output file with model-generated answers 
â”œâ”€â”€ requirements.txt     # Dependencies for the project
â””â”€â”€ README.md            # Project documentation
```

---

## ğŸ› ï¸ Tech Stack

**Language:** Python 3.13.5  
**Environment:** PyCharm IDE  

**Core Libraries:**
- `pandas` â€” data manipulation and analysis  
- `torch` â€” model interaction and computation  
- `matplotlib` / `seaborn` â€” visualization  
- `requests` / `openai` / `transformers` â€” model APIs and interfaces  
- `numpy`, `tqdm`, `json`, `os`, etc. â€” utilities


  ## âš™ï¸ Setup & Installation

### Prerequisites
- Python **3.13.5**
- PyCharm (recommended)
- A valid Hugging Face token and acceptance of Gemmaâ€™s policy

### Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/llm-linguistic-eval.git
   cd llm-linguistic-eval


## ğŸ“„ License
This projectâ€™s source code is licensed under the **MIT License** â€” see the [`LICENSE`](LICENSE) file for details.

âš ï¸ **Note on model usage:**
This repository interfaces with third-party large language models (LLMs) such as Gemma and others via official APIs or Hugging Face endpoints.  
Each model is governed by its own terms of service and licensing agreements.  
Users must:
- Obtain their own API tokens (e.g., from Hugging Face or OpenAI)
- Accept and comply with each modelâ€™s usage policy (e.g., [Gemma Model Policy](https://ai.google.dev/gemma/terms))
- Avoid redistributing model weights or outputs in violation of those terms

The authors of this repository do **not** claim ownership of or rights to any external model used in testing.
