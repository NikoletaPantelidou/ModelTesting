# ğŸ¤– PyCharm Misc Project - AI Prompt Processing System

Sistema completo para procesar prompts con modelos de IA, disponible en dos implementaciones:

- **ğŸ–¥ï¸ HuggingFace Local**: Ejecuta modelos localmente en tu GPU
- **â˜ï¸ Ollama Cloud**: Ejecuta modelos en la nube mediante API

## ğŸš€ Inicio RÃ¡pido

### Â¿Primera vez aquÃ­?

1. **Lee la guÃ­a de instalaciÃ³n**: [`INSTALLATION.md`](INSTALLATION.md)
2. **Elige tu implementaciÃ³n**:
   - **Principiantes** â†’ Ollama Cloud (5 minutos de setup)
   - **Avanzados con GPU** â†’ HuggingFace Local (1-2 horas de setup)

### Setup RÃ¡pido - Ollama Cloud

```bash
cd Ollama
setup.bat
set OLLAMA_API_KEY=tu_api_key_aqui
python test_connection.py
execute.bat
```

### Setup RÃ¡pido - HuggingFace Local

```bash
cd huggingFace
python -m venv .venv
.venv\Scripts\activate
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install transformers pandas huggingface_hub accelerate
python check_gpu.py
execute.bat
```

## ğŸ“ Estructura del Proyecto

```
PyCharmMiscProject/
â”‚
â”œâ”€â”€ ğŸ“– README.md                    â† EstÃ¡s aquÃ­
â”œâ”€â”€ ğŸ“– INSTALLATION.md              â† GuÃ­a de instalaciÃ³n completa
â”œâ”€â”€ ğŸ“– COMPARISON.md                â† ComparaciÃ³n HF vs Ollama
â”œâ”€â”€ ğŸ“– PROJECT_STRUCTURE.md         â† Estructura detallada
â”‚
â”œâ”€â”€ ğŸ–¥ï¸ huggingFace/                â† ImplementaciÃ³n Local (GPU)
â”‚   â”œâ”€â”€ script.py
â”‚   â”œâ”€â”€ script_models_config.py
â”‚   â”œâ”€â”€ execute.bat
â”‚   â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ answers/
â”‚   â””â”€â”€ logs/
â”‚
â””â”€â”€ â˜ï¸ Ollama/                     â† ImplementaciÃ³n Cloud (API)
    â”œâ”€â”€ script.py
    â”œâ”€â”€ script_models_config.py
    â”œâ”€â”€ test_connection.py
    â”œâ”€â”€ execute.bat
    â”œâ”€â”€ setup.bat
    â”œâ”€â”€ README.md
    â”œâ”€â”€ QUICKSTART.md
    â”œâ”€â”€ prompts/
    â”œâ”€â”€ answers/
    â””â”€â”€ logs/
```

## ğŸ¯ CaracterÃ­sticas Principales

### âœ¨ Ambas Implementaciones Incluyen

- âœ… **ParalelizaciÃ³n**: Procesa mÃºltiples prompts simultÃ¡neamente
- âœ… **Reinicio AutomÃ¡tico**: ContinÃºa desde donde se quedÃ³ si se interrumpe
- âœ… **Logging Completo**: Registros detallados de toda la ejecuciÃ³n
- âœ… **Manejo de Errores**: Reintentos automÃ¡ticos y recuperaciÃ³n
- âœ… **Guardado Incremental**: Guarda cada respuesta inmediatamente
- âœ… **Multi-modelo**: Procesa con varios modelos secuencialmente
- âœ… **Skip Completados**: Solo procesa respuestas pendientes

### ğŸ–¥ï¸ HuggingFace Local - CaracterÃ­sticas EspecÃ­ficas

- ğŸ”’ **Privacidad Total**: Los datos nunca salen de tu mÃ¡quina
- âš¡ **Baja Latencia**: Procesamiento local sin red
- ğŸ’ª **GPU Optimizado**: Usa CUDA con FP16 para mÃ¡xima eficiencia
- ğŸ›ï¸ **Control Total**: Acceso completo a parÃ¡metros del modelo
- ğŸ’° **Sin Costos de API**: Solo inversiÃ³n inicial en hardware

### â˜ï¸ Ollama Cloud - CaracterÃ­sticas EspecÃ­ficas

- ğŸš€ **Setup InstantÃ¡neo**: Listo en 5 minutos
- ğŸ’» **No Requiere GPU**: Funciona en cualquier PC
- ğŸ“ˆ **Escalable**: Sin limitaciones de hardware
- ğŸ”„ **Siempre Actualizado**: Modelos actualizados automÃ¡ticamente
- ğŸ’µ **Pago por Uso**: Sin inversiÃ³n inicial

## ğŸ“Š ComparaciÃ³n RÃ¡pida

| CaracterÃ­stica | HuggingFace Local | Ollama Cloud |
|---------------|-------------------|--------------|
| **Setup** | 1-2 horas | 5 minutos |
| **GPU Requerida** | SÃ­ (8GB+ VRAM) | No |
| **Costo Inicial** | Alto (GPU) | Ninguno |
| **Costo por Uso** | Solo electricidad | Requiere API key |
| **Privacidad** | Total | Depende de Ollama |
| **Velocidad** | Depende del HW | Consistente |
| **Modelos** | Miles (HuggingFace) | Cientos (Ollama) |

Ver comparaciÃ³n completa: [`COMPARISON.md`](COMPARISON.md)

## ğŸ“š DocumentaciÃ³n

| Documento | DescripciÃ³n |
|-----------|-------------|
| [`INSTALLATION.md`](INSTALLATION.md) | GuÃ­a completa de instalaciÃ³n para ambos sistemas |
| [`COMPARISON.md`](COMPARISON.md) | ComparaciÃ³n detallada HF vs Ollama |
| [`PROJECT_STRUCTURE.md`](PROJECT_STRUCTURE.md) | Estructura completa del proyecto |
| [`Ollama/README.md`](Ollama/README.md) | DocumentaciÃ³n especÃ­fica de Ollama |
| [`Ollama/QUICKSTART.md`](Ollama/QUICKSTART.md) | Inicio rÃ¡pido Ollama Cloud |

## ğŸ”§ Uso BÃ¡sico

### 1. Preparar tus Prompts

Crea un CSV en `prompts/example.csv`:

```csv
prompt;test_item
"John lives in Madrid. Mary lives in Paris, where does John live?";test_1
"The sky is blue. Grass is green, what color is the sky?";test_2
```

### 2. Configurar Modelos

**HuggingFace** (`huggingFace/script_models_config.py`):
```python
MODELS = [
    {"name": "mistralai/Mistral-7B-Instruct-v0.2", "use_qa_pipeline": False, "trust_remote_code": False},
    {"name": "microsoft/phi-4", "use_qa_pipeline": False, "trust_remote_code": False},
]
```

**Ollama** (`Ollama/script_models_config.py`):
```python
MODELS = [
    {"name": "llama3.2:latest"},
    {"name": "mistral:latest"},
]
```

### 3. Ejecutar

```bash
# HuggingFace
cd huggingFace
execute.bat

# Ollama
cd Ollama
execute.bat
```

### 4. Ver Resultados

Los resultados se guardan en:
- `answers/[modelo]_answers.csv` - Respuestas generadas
- `logs/execution_[timestamp].log` - Log de ejecuciÃ³n

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Ajustar Paralelismo

**HuggingFace** (`script.py` lÃ­nea 56):
```python
MAX_WORKERS = 1  # Para GPU: 1-2 recomendado
```

**Ollama** (`script.py` lÃ­nea 27):
```python
MAX_WORKERS = 2  # Para API: 2-4 recomendado
```

### Ajustar Temperatura

En ambos `script.py`:
```python
TEMPERATURE = 0.0  # 0.0 = determinista, 1.0 = creativo
```

### Ajustar Longitud de Respuesta

**HuggingFace**:
```python
max_new_tokens=300  # En funciÃ³n generate_answer()
```

**Ollama**:
```python
MAX_TOKENS = 300  # En configuraciÃ³n global
```

## ğŸ¯ Casos de Uso

### InvestigaciÃ³n AcadÃ©mica
**â†’ HuggingFace Local**
- Mayor control sobre experimentos
- Reproducibilidad garantizada
- Datos sensibles permanecen locales

### Prototipado RÃ¡pido
**â†’ Ollama Cloud**
- Setup instantÃ¡neo
- Sin preocupaciones tÃ©cnicas
- Ideal para validar ideas

### ProducciÃ³n Empresarial
**â†’ HÃ­brido**
- Ollama para modelos ligeros y consultas rÃ¡pidas
- HuggingFace para modelos especializados y alto volumen

### ComparaciÃ³n de Modelos
**â†’ Ambos**
- Ejecuta mismo prompt en mÃºltiples modelos
- Compara resultados fÃ¡cilmente
- Usa formato CSV compatible

## ğŸ› ï¸ Comandos Ãštiles

### Verificar InstalaciÃ³n

```bash
# HuggingFace
cd huggingFace
python check_gpu.py

# Ollama
cd Ollama
python test_connection.py
```

### Limpiar CachÃ©

```bash
# En cada carpeta
clean_cache.bat
```

### Ver Logs en Tiempo Real

```bash
# PowerShell
Get-Content logs\execution_*.log -Wait -Tail 20
```

### Reiniciar desde Cero

```bash
# Eliminar respuestas para volver a procesar
rmdir /s /q answers
mkdir answers
```

## ğŸ› SoluciÃ³n de Problemas

### HuggingFace: "CUDA not available"

```bash
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
python check_gpu.py
```

### HuggingFace: "Out of memory"

```python
MAX_WORKERS = 1  # Reducir paralelismo
```

### Ollama: "API key not set"

```bash
set OLLAMA_API_KEY=tu_api_key_aqui
# O permanentemente:
setx OLLAMA_API_KEY "tu_api_key_aqui"
```

### Ollama: "Connection timeout"

```python
MAX_WORKERS = 1  # Reducir llamadas simultÃ¡neas
```

Ver guÃ­a completa: [`INSTALLATION.md`](INSTALLATION.md#-soluciÃ³n-de-problemas)

## ğŸŒŸ Ejemplos de Output

### Archivo de Entrada (`prompts/example.csv`)
```csv
prompt;test_item
"John lives in Madrid, where does John live?";geo_test_1
```

### Archivo de Salida (`answers/llama3.2-latest_answers.csv`)
```csv
prompt;test_item;answer
"John lives in Madrid, where does John live?";geo_test_1;"John lives in Madrid."
```

### Log de EjecuciÃ³n (`logs/execution_20251031_192700.log`)
```
2025-10-31 19:27:00 - INFO - [INFO] Loading model: llama3.2:latest
2025-10-31 19:27:05 - INFO - [INFO] Processing row 1/1 - test_item: geo_test_1
2025-10-31 19:27:05 - INFO - [PROMPT] John lives in Madrid, where does John live?
2025-10-31 19:27:08 - INFO - [OK] Row 1 completed
2025-10-31 19:27:08 - INFO - [PROGRESS] llama3.2:latest - 1/1 rows completed
```

## ğŸ”„ Flujo de Trabajo Recomendado

```mermaid
graph TD
    A[Preparar prompts CSV] --> B{Â¿Tienes GPU?}
    B -->|SÃ­| C[HuggingFace Local]
    B -->|No| D[Ollama Cloud]
    C --> E[Ejecutar script]
    D --> E
    E --> F[Revisar answers/]
    F --> G{Â¿Satisfecho?}
    G -->|No| H[Ajustar config]
    H --> E
    G -->|SÃ­| I[AnÃ¡lisis de resultados]
```

## ğŸ“ˆ Mejores PrÃ¡cticas

### Para HuggingFace Local

1. **Usa FP16** (ya configurado) para ahorrar VRAM
2. **MAX_WORKERS = 1** para evitar OOM errors
3. **Libera cachÃ©** entre modelos (ya implementado)
4. **Monitorea GPU** con `nvidia-smi` durante ejecuciÃ³n
5. **Usa modelos cuantizados** si tienes VRAM limitada

### Para Ollama Cloud

1. **MAX_WORKERS = 2-4** para aprovechar paralelismo
2. **Maneja rate limits** reduciendo workers si es necesario
3. **Implementa backoff** para errores de red (ya implementado)
4. **Monitorea costos** de API regularmente
5. **Cachea resultados** para evitar llamadas duplicadas

### Para Ambos

1. **Versiona tus prompts** con Git
2. **Guarda logs** para debugging posterior
3. **Documenta configuraciones** usadas
4. **Compara resultados** entre modelos
5. **Automatiza evaluaciÃ³n** de respuestas

## ğŸ¤ Contribuir

Este es un proyecto de investigaciÃ³n. Sugerencias de mejora:

1. **Fork el proyecto**
2. **Crea branch** para tu feature
3. **Implementa mejoras**
4. **Documenta cambios**
5. **Crea pull request**

## ğŸ“ Licencia

Este proyecto es para uso acadÃ©mico y de investigaciÃ³n.

## ğŸ†˜ Soporte

### Problemas TÃ©cnicos

- **HuggingFace**: Revisa `check_gpu.py` output
- **Ollama**: Ejecuta `test_connection.py`
- **General**: Revisa logs en `logs/`

### Recursos Externos

- **PyTorch + CUDA**: https://pytorch.org/get-started/locally/
- **HuggingFace Hub**: https://huggingface.co/docs
- **Ollama Cloud**: https://docs.ollama.com/cloud
- **Transformers**: https://huggingface.co/docs/transformers

### DocumentaciÃ³n del Proyecto

- [INSTALLATION.md](INSTALLATION.md) - InstalaciÃ³n completa
- [COMPARISON.md](COMPARISON.md) - HF vs Ollama
- [PROJECT_STRUCTURE.md](PROJECT_STRUCTURE.md) - Estructura
- [Ollama/QUICKSTART.md](Ollama/QUICKSTART.md) - Inicio rÃ¡pido

## ğŸ“ Para Aprender MÃ¡s

### Sobre Modelos LLM
- https://huggingface.co/learn
- https://www.deeplearning.ai/courses/

### Sobre CUDA y GPU Computing
- https://developer.nvidia.com/cuda-education
- https://pytorch.org/tutorials/

### Sobre APIs de IA
- https://docs.ollama.com/
- https://platform.openai.com/docs/

---

## â­ CaracterÃ­sticas Destacadas

### ğŸ”„ Reinicio Inteligente
Si el script se interrumpe, simplemente ejecÃºtalo de nuevo. DetectarÃ¡ automÃ¡ticamente quÃ© respuestas ya estÃ¡n completadas y solo procesarÃ¡ las pendientes.

### ğŸ’¾ Guardado Incremental
Cada respuesta se guarda inmediatamente despuÃ©s de generarse, nunca perderÃ¡s progreso por errores o interrupciones.

### ğŸ“Š Multi-modelo Eficiente
Procesa con mÃºltiples modelos secuencialmente, liberando memoria entre cada uno para mÃ¡xima eficiencia.

### ğŸ¯ Logging Detallado
Sabe exactamente quÃ© estÃ¡ pasando en cada momento con logs completos y estructurados.

### ğŸ”€ ParalelizaciÃ³n Inteligente
Usa ThreadPoolExecutor para procesar mÃºltiples prompts simultÃ¡neamente sin sobrecargar recursos.

---

**Â¿Listo para empezar?**

```bash
# OpciÃ³n rÃ¡pida: Ollama Cloud
cd Ollama
setup.bat

# OpciÃ³n avanzada: HuggingFace Local
cd huggingFace
python check_gpu.py
```

Â¡Buena suerte con tu procesamiento de prompts! ğŸš€

